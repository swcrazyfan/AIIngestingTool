# Data Mapping: VideoIngestOutput to app_data.clips Table

This document details the mapping from the `VideoIngestOutput` Pydantic model (and its constituent data sources from various pipeline tasks) to the `app_data.clips` table in the DuckDB database. The primary mapping logic resides in `video_ingest_tool.database.duckdb.mappers.prepare_clip_data_for_db`.

**Pydantic Model Source:** `video_ingest_tool.models.VideoIngestOutput` (constructed by `create_model_step`)
**Database Table:** `app_data.clips`
**Mapper Function:** `prepare_clip_data_for_db(video_output: VideoIngestOutput, embeddings: Dict, ai_selected_thumbnail_metadata: Optional[List[Dict]])`

## `app_data.clips` Table Column Mapping

| DB Column (`app_data.clips`) | Data Type     | Source in `VideoIngestOutput` (via `video_output` object) & Notes                                                                                                                                                              | Originating Task(s) / Key in `data` dict for `create_model_step` (unless specified for mapper) |
|------------------------------|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| `id`                         | `UUID`        | `video_output.id` (Generated by `VideoIngestOutput` model's default factory or from `create_model_step` if an ID was part of its input `data` dict). Mapper ensures a UUID.                                                    | `create_model_step` (from `VideoIngestOutput` default or input `data['id']`)                     |
| `local_path`                 | `VARCHAR`     | `video_output.file_info.local_path` (Previously `file_path`, renamed in `FileInfo` model). This is the primary path to the media file.                                                                                         | Initial `file_path` passed to `process_video_file_task` -> `data['file_path']`.                  |
| `file_name`                  | `VARCHAR`     | `video_output.file_info.file_name`                                                                                                                                                                                               | `generate_checksum_step` -> `data['file_name']`.                                                 |
| `file_checksum`              | `VARCHAR`     | `video_output.file_info.file_checksum`                                                                                                                                                                                           | `generate_checksum_step` -> `data['checksum']`.                                                  |
| `file_size_bytes`            | `BIGINT`      | `video_output.file_info.file_size_bytes`                                                                                                                                                                                         | `generate_checksum_step` -> `data['file_size_bytes']`. (Also available from mediainfo/ffprobe but checksum step is simpler source for `FileInfo`). |
| `duration_seconds`           | `DOUBLE`      | `video_output.video.duration_seconds`                                                                                                                                                                                            | `create_model_step` (from `data['mediainfo_data']` or `data['ffprobe_data']`).                   |
| `created_at`                 | `TIMESTAMP`   | `video_output.file_info.created_at` (Mapper defaults to `datetime.utcnow()` if None).                                                                                                                                            | `create_model_step` (from `data['exiftool_data']` or `data['mediainfo_data']`).                 |
| `processed_at`               | `TIMESTAMP`   | `video_output.file_info.processed_at` (Mapper defaults to `datetime.utcnow()` if None).                                                                                                                                          | `FileInfo` model default factory or set by `create_model_step` (`datetime.now()`).               |
| `updated_at`                 | `TIMESTAMP`   | Set by mapper to `datetime.utcnow()` upon mapping.                                                                                                                                                                               | `mappers.py` (current timestamp).                                                                |
| `width`                      | `INTEGER`     | `video_output.video.resolution.width`                                                                                                                                                                                             | `create_model_step` (from `data['mediainfo_data']` or `data['ffprobe_data']`).                   |
| `height`                     | `INTEGER`     | `video_output.video.resolution.height`                                                                                                                                                                                            | `create_model_step` (from `data['mediainfo_data']` or `data['ffprobe_data']`).                   |
| `frame_rate`                 | `DOUBLE`      | `video_output.video.frame_rate`                                                                                                                                                                                                 | `create_model_step` (from `data['mediainfo_data']` or `data['ffprobe_data']`).                   |
| `codec`                      | `VARCHAR`     | `video_output.video.codec.name`                                                                                                                                                                                                 | `create_model_step` (from `data['mediainfo_data']`, `data['ffprobe_data']`, or `data['codec_params']`). |
| `container`                  | `VARCHAR`     | `video_output.video.container`                                                                                                                                                                                                  | `create_model_step` (from `data['mediainfo_data']` or `data['ffprobe_data']`).                   |
| `camera_make`                | `VARCHAR`     | `video_output.camera.make`                                                                                                                                                                                                      | `create_model_step` (from `data['exiftool_data']` or `data['extended_exif_data']`).             |
| `camera_model`               | `VARCHAR`     | `video_output.camera.model`                                                                                                                                                                                                     | `create_model_step` (from `data['exiftool_data']` or `data['extended_exif_data']`).             |
| `camera_details`             | `JSON`        | `video_output.camera.model_dump_json()` (Full `CameraDetails` object)                                                                                                                                                           | `create_model_step` (from various exif, extended_exif, and focal length step data).            |
| `content_category`           | `VARCHAR`     | `video_output.analysis.ai_analysis.summary.content_category` (if `ai_analysis` and `summary` exist)                                                                                                                             | `create_model_step` (from `data['ai_analysis_summary']['content_category']`).                    |
| `content_summary`            | `TEXT`        | `video_output.analysis.content_summary` (This is from `AnalysisDetails`, populated by `create_model_step` from `data['ai_analysis_summary']['overall_summary']`)                                                               | `create_model_step` (from `data['ai_analysis_summary']['overall_summary']`).                     |
| `content_tags`               | `VARCHAR[]`   | `video_output.analysis.content_tags` (List of strings from `AnalysisDetails`, populated by `create_model_step` from AI analysis data)                                                                                           | `create_model_step` (derived from `data['ai_analysis_summary']` & `data['full_ai_analysis_data']`). |
| `searchable_content`         | `TEXT`        | Generated by `mappers._generate_searchable_content(video_output)`. Includes `file_name`, `analysis.content_summary`, `analysis.content_tags`, `ai_analysis.summary.overall`, etc.                                              | `mappers.py` (derived from multiple `VideoIngestOutput` fields).                                 |
| `full_transcript`            | `TEXT`        | `video_output.analysis.ai_analysis.audio_analysis.transcript.full_text` (if available)                                                                                                                                          | `create_model_step` (from `data['full_ai_analysis_data']['audio_analysis']['transcript']`).      |
| `transcript_preview`         | `VARCHAR`     | `video_output.analysis.ai_analysis.audio_analysis.transcript.full_text` (Mapper currently copies `full_text` here).                                                                                                               | `create_model_step` (as above, then used by mapper).                                             |
| `transcript_segments_json`   | `JSON`        | JSON dump of `video_output.analysis.ai_analysis.audio_analysis.transcript.segments` (List of `TranscriptSegment` models).                                                                                                        | `create_model_step` (from `data['full_ai_analysis_data']['audio_analysis']['transcript']`).      |
| `thumbnails`                 | `VARCHAR[]`   | `video_output.thumbnails` (List of strings, paths to initially generated thumbnails).                                                                                                                                           | `generate_thumbnails_step` -> `data['thumbnail_paths']`.                                         |
| `primary_thumbnail_path`     | `VARCHAR`     | Derived by mapper from the `ai_selected_thumbnail_metadata` argument (path of rank 1 thumbnail).                                                                                                                                | `ai_thumbnail_selection_step` -> `data['ai_thumbnail_metadata']` (passed to mapper).             |
| `ai_selected_thumbnails_json`| `JSON`        | JSON dump of `ai_selected_thumbnail_metadata` argument passed to mapper.                                                                                                                                                        | `ai_thumbnail_selection_step` -> `data['ai_thumbnail_metadata']` (passed to mapper).             |
| `technical_metadata`         | `JSON`        | `video_output.video.codec.model_dump_json()` (Full `VideoCodecDetails` object).                                                                                                                                                 | `create_model_step` (from various mediainfo, ffprobe, codec_params data).                      |
| `audio_tracks`               | `JSON`        | JSON dump of `video_output.audio_tracks` (List of `AudioTrack` models).                                                                                                                                                         | `extract_audio_step` -> `data['audio_tracks']`.                                                  |
| `subtitle_tracks`            | `JSON`        | JSON dump of `video_output.subtitle_tracks` (List of `SubtitleTrack` models).                                                                                                                                                   | `extract_subtitle_step` -> `data['subtitle_tracks']`.                                            |
| `full_ai_analysis_json`      | `JSON`        | `video_output.analysis.ai_analysis.model_dump_json()` (Full `ComprehensiveAIAnalysis` object).                                                                                                                                  | `ai_video_analysis_step` -> `data['full_ai_analysis_data']`.                                     |
| `summary_embedding`          | `FLOAT[1024]` | From `embeddings['summary_embedding']` argument passed to mapper. (Assumes `generate_embeddings_step` is refactored to return embeddings).                                                                                      | `generate_embeddings_step` -> `data['embeddings']['summary_embedding']` (passed to mapper).      |
| `keyword_embedding`          | `FLOAT[1024]` | From `embeddings['keyword_embedding']` argument passed to mapper. (Assumes `generate_embeddings_step` is refactored to return embeddings).                                                                                      | `generate_embeddings_step` -> `data['embeddings']['keyword_embedding']` (passed to mapper).      |
| `thumbnail_1_embedding`      | `FLOAT[768]`  | From `embeddings['thumbnail_1_embedding']` argument passed to mapper. (Assumes `generate_embeddings_step` is refactored).                                                                                                      | `generate_embeddings_step` -> `data['embeddings']['thumbnail_1_embedding']` (passed to mapper).   |
| `thumbnail_2_embedding`      | `FLOAT[768]`  | From `embeddings['thumbnail_2_embedding']` argument passed to mapper. (Assumes `generate_embeddings_step` is refactored).                                                                                                      | `generate_embeddings_step` -> `data['embeddings']['thumbnail_2_embedding']` (passed to mapper).   |
| `thumbnail_3_embedding`      | `FLOAT[768]`  | From `embeddings['thumbnail_3_embedding']` argument passed to mapper. (Assumes `generate_embeddings_step` is refactored).                                                                                                      | `generate_embeddings_step` -> `data['embeddings']['thumbnail_3_embedding']` (passed to mapper).   |

## Notes on Data Flow within `process_video_file_task`:

The `process_video_file_task` in `video_ingest_tool.flows.prefect_flows` orchestrates the execution of various sub-tasks. Each sub-task typically returns a dictionary, which is then merged into a central `data` dictionary that accumulates all findings.

1.  **Initial Data:** `file_path` (renamed to `local_path` in `FileInfo`), `thumbnails_dir`, `config`, etc.
2.  **Extraction Tasks:**
    *   `generate_checksum_step`: Adds `checksum`, `file_name`, `file_size_bytes`.
    *   `extract_mediainfo_step`: Adds `mediainfo_data`.
    *   `extract_ffprobe_step`: Adds `ffprobe_data`.
    *   `extract_exiftool_step`: Adds `exiftool_data`.
    *   `extract_extended_exif_step`: Adds `extended_exif_data`.
    *   `extract_codec_step`: Adds `codec_params`.
    *   `extract_hdr_step`: Adds `hdr_data`.
    *   `extract_audio_step`: Adds `audio_tracks` (list of dicts).
    *   `extract_subtitle_step`: Adds `subtitle_tracks` (list of dicts).
3.  **Analysis Tasks:**
    *   `generate_thumbnails_step`: Adds `thumbnail_paths` (list of general thumbnail file paths).
    *   `analyze_exposure_step`: Adds `exposure_data` (based on the first general thumbnail).
    *   `detect_focal_length_step`: Adds `focal_length_source` and potentially `focal_length_category`.
    *   `video_compression_step`: Adds `compressed_video_path`.
    *   `ai_video_analysis_step`: Adds `ai_analysis_summary`, `ai_analysis_file_path`, `full_ai_analysis_data`.
    *   `ai_thumbnail_selection_step`: Adds `ai_thumbnail_paths` and `ai_thumbnail_metadata`.
4.  **Storage & Model Creation:**
    *   `create_model_step`: Takes the accumulated `data` dictionary and constructs the `VideoIngestOutput` Pydantic model. This step internally performs consolidation logic (similar to the unused `consolidate_metadata_step`) by selecting and structuring data from the various `*_data` sub-dictionaries to fit the `VideoIngestOutput` schema. The `VideoIngestOutput` object is then returned as `data['model']`.
    *   **(Future) `database_storage_step` (DuckDB version):** Will take `data['model']` (the `VideoIngestOutput`), `data['embeddings']` (once `generate_embeddings_step` returns it), and `data['ai_thumbnail_metadata']`. It will call `mappers.prepare_clip_data_for_db` and then `crud.upsert_clip_data`. It needs to return `{'clip_id': ...}`.
    *   **`generate_embeddings_step`:** Currently stores embeddings directly. **Needs refactoring** to return a dictionary of embeddings (e.g., `{'summary_embedding': ..., 'keyword_embedding': ..., 'thumbnail_1_embedding': ...}`) which will be added to the `data` dictionary as `data['embeddings']`. This `data['embeddings']` is then passed to the mapper.

This detailed trace clarifies the origin of each piece of data that ultimately populates the `VideoIngestOutput` model and, through the mapper, the `app_data.clips` database table.